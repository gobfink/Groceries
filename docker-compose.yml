version: '3.5'
services:
  splash-middleware:
     hostname: splash-middleware
     image: scrapinghub/splash
     command: --disable-private-mode
     ports:
     - "5023:5023"
     - "8050:8050"
     - "8051:8051"
     networks:
     - backend
  scrapy:
     volumes:
     - ./scrapy/code:/code
     -  /tmp/.X11-unix:/tmp/.X11-unix
     build:
       context: scrapy/
       dockerfile: Dockerfile.debug
       shm_size: '8gb'
     hostname: scrapy
     environment:
        - "DISPLAY"
        - "PYTHONPATH=/code/groceries"
     depends_on:
     -  splash-middleware
     -  db
     -  adminer
     networks:
     - backend
     entrypoint:
     - "/bin/bash"
     working_dir: /code/groceries/spiders/walmart
  harris-teeter-urls:
     healthcheck:
       test: ["CMD", "/opt/healthcheck.sh", "harris-teeter", "url"]
       interval: 30s
       start_period: 90s
       retries: 1
     volumes:
     - ./scrapy/code:/code
     build:
       context: scrapy/
       dockerfile: Dockerfile.selenium.runtime
       shm_size: '8gb'
     hostname: wegmans-scrapy
     environment:
        - "PYTHONPATH=/code/groceries"
     depends_on:
     -  db
     -  adminer
     networks:
     - backend
     entrypoint: "/usr/local/bin/scrapy runspider urlScraper.py"
     working_dir: /code/groceries/spiders/harris-teeter
     restart: unless-stopped
  harris-teeter:
     healthcheck:
       test: ["CMD", "/opt/healthcheck.sh", "harris-teeter", "groceries"]
       interval: 30s
       start_period: 90s
       retries: 1
     volumes:
     - ./scrapy/code:/code
     build:
       context: scrapy/
       dockerfile: Dockerfile.selenium.runtime
       shm_size: '6gb'
     hostname: harristeeter-scrapy
     environment:
        - "PYTHONPATH=/code/groceries"
     depends_on:
     -  db
     -  adminer
     networks:
     - backend
     entrypoint: "/usr/local/bin/scrapy runspider groceryScraper.py"
     working_dir: /code/groceries/spiders/harris-teeter
     restart: unless-stopped
  wegmans:
     volumes:
     - ./scrapy/code:/code
     build:
       context: scrapy/
       dockerfile: Dockerfile.selenium.runtime
       shm_size: '3gb'
     hostname: wegmans-scrapy
     environment:
        - "PYTHONPATH=/code/groceries"
     depends_on:
     -  db
     -  adminer
     networks:
     - backend
     entrypoint: "/usr/local/bin/scrapy runspider scraper.py"
     working_dir: /code/groceries/spiders/wegmans
     restart: unless-stopped
  safeway:
     healthcheck:
       test: ["CMD", "/opt/healthcheck.sh", "safeway", "groceries"]
       interval: 30s
       start_period: 90s
       retries: 1
     volumes:
     - ./scrapy/code:/code
     build:
       context: scrapy/
       dockerfile: Dockerfile.selenium.runtime
       shm_size: '4gb'
     hostname: safeway-groceries
     environment:
        - "PYTHONPATH=/code/groceries"
     depends_on:
     -  db
     -  adminer
     networks:
     - backend
     entrypoint: "/usr/local/bin/scrapy runspider groceryScraper.py"
     working_dir: /code/groceries/spiders/safeway
     restart: unless-stopped
  safeway-urls:
     healthcheck:
       test: ["CMD", "/opt/healthcheck.sh", "safeway", "url"]
       interval: 60s
       start_period: 200s
       retries: 1
     volumes:
     - ./scrapy/code:/code
     build: 
       context: scrapy/
       dockerfile: Dockerfile.selenium.runtime
       shm_size: '6gb'
     hostname: safeway-urls
     environment: 
        - "PYTHONPATH=/code/groceries"
     depends_on:
     -  db
     -  adminer
     networks:
     - backend
     entrypoint: "/usr/local/bin/scrapy runspider urlScraper.py"
     working_dir: /code/groceries/spiders/safeway
     restart: unless-stopped
  safeway-groceries:
     healthcheck:
       test: ["CMD", "/opt/healthcheck.sh", "safeway", "grocery"]
       interval: 60s
       start_period: 200s
       retries: 1
     volumes:
     - ./scrapy/code:/code
     build: 
       context: scrapy/
       dockerfile: Dockerfile.selenium.runtime
       shm_size: '6gb'
     hostname: safeway-groceries
     environment: 
        - "PYTHONPATH=/code/groceries"
     depends_on:
     -  db
     -  adminer
     networks:
     - backend
     entrypoint: "/usr/local/bin/scrapy runspider groceryScraper.py"
     working_dir: /code/groceries/spiders/safeway
     restart: unless-stopped
  lidl-urls:
    healthcheck:
       test: ["CMD", "/opt/healthcheck.sh", "lidl", "url"]
       interval: 30s
       start_period: 90s
       retries: 1
    volumes:
    - ./scrapy/code:/code
    build:
      context: scrapy/
      dockerfile: Dockerfile.runtime
    hostname: lidl-urls
    depends_on:
      - splash-middleware
      - db
      - adminer
    networks:
      - backend
    working_dir: /code/groceries/spiders/lidl
    entrypoint: "/usr/local/bin/scrapy runspider urlScraper.py"
    environment:
      PYTHONPATH: /code/groceries/
    restart: unless-stopped
  lidl-groceries:
    healthcheck:
       test: ["CMD", "/opt/healthcheck.sh", "lidl", "groceries"]
       interval: 30s
       start_period: 90s
       retries: 1
    volumes:
    - ./scrapy/code:/code
    build:
      context: scrapy/
      dockerfile: Dockerfile.runtime
    hostname: lidl-groceries
    depends_on:
      - splash-middleware
      - db
      - adminer
    networks:
      - backend
    working_dir: /code/groceries/spiders/lidl
    entrypoint: "/usr/local/bin/scrapy runspider groceryScraper.py"
    environment:
      PYTHONPATH: /code/groceries/
    restart: unless-stopped
  lidl:
    volumes:
    - ./scrapy/code:/code
    build:
      context: scrapy/
      dockerfile: Dockerfile.runtime
    hostname: lidl-scrapy
    depends_on:
    - splash-middleware
    - db
    - adminer
    networks:
    - backend
    working_dir: /code/groceries/spiders/lidl
    entrypoint: "/usr/local/bin/scrapy runspider scraper.py"
    environment:
      PYTHONPATH: /code/groceries/
    restart: unless-stopped
  walmart-urls:
    healthcheck:
       test: ["CMD", "/opt/healthcheck.sh", "walmart", "urls"]
       interval: 30s
       start_period: 90s
       retries: 1
    volumes:
    - ./scrapy/code:/code
    build:
      context: scrapy/
      dockerfile: Dockerfile.selenium.runtime
    hostname: walmart-urls
    depends_on:
    - splash-middleware
    - db
    - adminer
    networks:
    - backend
    working_dir: /code/groceries/spiders/walmart
    entrypoint: "/usr/local/bin/scrapy runspider urlScraper.py"
    environment:
      PYTHONPATH: /code/groceries/
    restart: unless-stopped
  walmart:
    volumes:
    - ./scrapy/code:/code
    build:
      context: scrapy/
      dockerfile: Dockerfile.runtime
    hostname: walmart-scrapy
    depends_on:
    - splash-middleware
    - db
    - adminer
    networks:
    - backend
    working_dir: /code/groceries/spiders/walmart
    entrypoint: "/usr/local/bin/scrapy runspider scraper.py"
    environment:
      PYTHONPATH: /code/groceries/
    restart: unless-stopped
  db:
    hostname: db
    image: mysql
    command: --default-authentication-plugin=mysql_native_password
    restart: unless-stopped
    networks:
      - backend
    environment:
      MYSQL_DATABASE: db
      MYSQL_USER: root
      MYSQL_PASSWORD: example
      MYSQL_ROOT_PASSWORD: example
    volumes:
      #- ./schema:/docker-entrypopint-initdb.d
      - mysql-data:/var/lib/mysql
    ports:
      - "3306:3306"
  adminer:
    hostname: adminer
    image: adminer
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      - db
    ports:
      - "8081:8080"
  flask:
    hostname: flask
    build: './flask/'
    restart: unless-stopped
    volumes:
      - ./flask/code:/code
      - ./upload:/code/app/upload
    ports:
      - "8080:80"
    networks:
      - backend
      - frontend
    environment:
      FLASK_CONFIG: development
      FLASK_APP: run.py
    depends_on:
      - db
networks:
  frontend:
  backend:

volumes:
   mysql-data:
